<!DOCTYPE HTML>
<html>

<head>
    <title>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=1000">
    <link rel="stylesheet" href="assets/css/main.css" />
    <link rel="icon" href="assets/audio-icon.jpg">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LS6L6LB7RX"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-LS6L6LB7RX');
    </script>

    <meta property="og:url" content="https://umi-gripper.github.io" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data" />
    <meta property="og:description" content="" />


</head>

<body id="top">


    <!-- Main -->
    <div id="main"
        style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">

        <h1 style="text-align: center; margin-bottom: 0; color: #e0218a; font-size: 300%">ManiWAV:</h1>
        <h2 style="text-align: center; white-space: nowrap; font-size: 200%">Learning Robot <span
                style="color:#e0218a">Mani</span>pulation
            from In-the-<span style="color:#e0218a">W</span>ild <span style="color:#e0218a">A</span>udio-<span
                style="color:#e0218a">V</span>isual Data</h2>

        <div class="box alt" style="margin-bottom: 1em;">
            <div class="row 50% uniform" style="width: 100%; color: black;">
                <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                    Human Demonstration with Audio-Visual Feedback
                </div>

                <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                    Robot Policy Rollout
                </div>
            </div>
            <div class="row 50% uniform" style="width: 100%;">
                <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                    <span class="image fit">
                        <video controls autoplay muted loop style="width: 100%; margin-right: 5%;">
                            <source src="videos/demo.mp4" type="video/mp4">
                        </video>
                    </span>
                    <!-- Dish Washing Data Collection-->
                </div>

                <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 50%">
                    <span class="image fit">
                        <video controls autoplay muted loop style="width: 100%; margin-top:5%; margin-right: 5%;">
                            <source src="videos/white-pan-itw.mp4" type="video/mp4">
                        </video>
                        <!-- <div style="position: absolute; top: 2%; right: 1%">
                                    <p style="color: white; text-align: center; font-size: 2em">1x</p>
                                </div> -->
                    </span>
                    <!-- Dish Washing -->
                </div>
            </div>
        </div>

        <p style="color: black;">Audio signals provide rich information for the robot interaction and object properties
            through contact. These information can surprisingly ease the learning of contact-rich robot manipulation
            skills, especially when the visual information alone is ambiguous or incomplete. However, the usage of audio
            data in robot manipulation has been constrained to teleoperated demonstrations collected by either attaching
            a microphone to the robot or object, which significantly limits its usage in robot learning pipelines. In
            this work, we introduce ManiWAV: an 'ear-in-hand' data collection device to collect in-the-wild human
            demonstrations with synchronous audio and visual feedback, and a corresponding policy interface to learn
            robot manipulation policy directly from the demonstrations. We demonstrate the capabilities of our system
            through four contact-rich manipulation tasks that require either passively sensing the contact events and
            modes, or actively sensing the object surface materials and states. In addition, we show that our system can
            generalize to unseen in-the-wild environments, by learning from diverse in-the-wild human demonstrations.
        </p>

        <hr>
        <h3>Technical Summary Video</h3>
        <div class="row 50% uniform" style="width: 100%; margin-top: 0.5em;">
            <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 100%">
                <span class="image fit">
                    <video controls muted loop style="width: 80%; margin-top: -2%;">
                        <source src="videos/summary.mp4" type="video/mp4">
                    </video>
                </span>
            </div>
        </div>

        <hr>
        <h3>Capability Experiments</h3>
        <h4> (a) Wiping Whiteboard ðŸª§</h4>
        The robot is tasked to wipe a shape (e.g. heart, square) drawn on a whiteboard. The robot can start in any
        initial configuration above the whiteboard and grasp an eraser in parallel to the board. The main challenge of
        the task is that the robot needs to exert a reasonable amount of contact force on the whiteboard while moving
        the eraser along the shape. Watch the below video for details of the task and ablations.
        <div class="row 50% uniform" style="width: 100%; margin-top: 0.5em;">
            <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 100%">
                <span class="image fit">
                    <video controls muted loop style="width: 80%; margin-top: -2%;">
                        <source src="videos/tasks/wipe-summary-compressed.mp4" type="video/mp4">
                    </video>
                </span>
            </div>
        </div>

        <br><br>
        <h4> (b) Flipping Bagel ðŸ¥¯</h4>
        The robot is tasked to flip a bagel in a pan from facing down to facing upward using a spatula. To perform this
        task successfully, the robot needs to sense and switch between different contact modes -- precisely insert the
        spatula between the bagel and the pan, maintain the contact while sliding, and start to tilt up the spatula when
        the bagel is in contact with the edge of the pan. Watch the below video for details of the task and ablations.
        <div class="row 50% uniform" style="width: 100%; margin-top: 0.5em;">
            <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 100%">
                <span class="image fit">
                    <video controls muted loop style="width: 80%; margin-top: -2%;">
                        <source src="videos/tasks/flip-summary-compressed.mp4" type="video/mp4">
                    </video>
                </span>
            </div>
        </div>

        <br><br>
        <h4> (c) Pouring ðŸŽ² </h4>
        The robot is tasked to pick up the white cup and pour dice out to the pink cup if the white cup is not empty.
        When finish pouring, the robot needs to place the empty cup down to a designated location.
        The challenge of the task is that the robot cannot observe whether there are dice in the cup or not given the
        camera view point both before and after the pouring action. Therefore it needs to leverage a `shaking' action to
        verify the cup is empty or not. Watch the below video for details of the task and ablations.
        <div class="row 50% uniform" style="width: 100%; margin-top: 0.5em;">
            <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 100%">
                <span class="image fit">
                    <video controls muted loop style="width: 80%; margin-top: -2%;">
                        <source src="videos/tasks/pour-summary-compressed.mp4" type="video/mp4">
                    </video>
                </span>
            </div>
        </div>

        <br><br>
        <h4> (d) Taping Wires with Velcro Tape âž° </h4>
        The robot is tasked to choose the `hook' tape from several tapes (either 'hook' or 'loop') and strap wires by
        attaching the 'hook' tape to a `loop' tape underneath the wires.
        We collect 200 demonstrations in total, with a 'sliding' primitive where we use the tip of the gripper finger to
        slide along the tape. Watch the below video for details of the task and ablations.
        <div class="row 50% uniform" style="width: 100%; margin-top: 0.5em;">
            <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 100%">
                <span class="image fit">
                    <video controls muted loop style="width: 80%; margin-top: -2%;">
                        <source src="videos/tasks/tape-summary-compressed.mp4" type="video/mp4">
                    </video>
                </span>
            </div>
        </div>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.poptrox.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
</body>

</html>